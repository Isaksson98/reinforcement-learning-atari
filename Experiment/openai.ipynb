{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ENV_NAME = 'CartPole-v0'\n",
    "env = gym.make(ENV_NAME)\n",
    "nb_actions = env.action_space.n #number of actios we can take\n",
    "nb_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "observation = env.reset()\n",
    "for _ in range(1000):\n",
    "  env.render()\n",
    "  action = env.action_space.sample() # your agent here (this takes random actions)\n",
    "  observation, reward, done, info = env.step(action)\n",
    "\n",
    "  if done:\n",
    "    observation = env.reset()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Activation\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(env):    \n",
    "    model = Sequential()\n",
    "    model.add(Flatten(input_shape=(1,) + env.observation_space.shape))\n",
    "    \n",
    "    #model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(16, activation='relu'))\n",
    "    model.add(Dense(nb_actions))\n",
    "    model.add(Activation('linear'))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 4)                 0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 16)                80        \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 34        \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 2)                 0         \n",
      "=================================================================\n",
      "Total params: 114\n",
      "Trainable params: 114\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl.agents import DQNAgent\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.policy import LinearAnnealedPolicy, EpsGreedyQPolicy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 5000 steps ...\n",
      "    9/5000: episode: 1, duration: 0.164s, episode steps:   9, steps per second:  55, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: --, mae: --, mean_q: --\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\filip\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\rl\\memory.py:37: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
      "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   18/5000: episode: 2, duration: 1.183s, episode steps:   9, steps per second:   8, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.436140, mae: 0.546924, mean_q: -0.001049\n",
      "   27/5000: episode: 3, duration: 0.105s, episode steps:   9, steps per second:  86, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.293244, mae: 0.507683, mean_q: 0.191356\n",
      "   36/5000: episode: 4, duration: 0.084s, episode steps:   9, steps per second: 108, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.194857, mae: 0.469597, mean_q: 0.390379\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\filip\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\rl\\memory.py:37: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
      "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n",
      "c:\\users\\filip\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\rl\\memory.py:37: UserWarning: Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!\n",
      "  warnings.warn('Not enough entries to sample without replacement. Consider increasing your warm-up phase to avoid oversampling!')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   46/5000: episode: 5, duration: 0.089s, episode steps:  10, steps per second: 112, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.145163, mae: 0.442836, mean_q: 0.575358\n",
      "   55/5000: episode: 6, duration: 0.073s, episode steps:   9, steps per second: 123, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.108247, mae: 0.453523, mean_q: 0.802840\n",
      "   65/5000: episode: 7, duration: 0.076s, episode steps:  10, steps per second: 131, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.900 [0.000, 1.000],  loss: 0.100219, mae: 0.485305, mean_q: 0.983688\n",
      "   76/5000: episode: 8, duration: 0.091s, episode steps:  11, steps per second: 120, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.909 [0.000, 1.000],  loss: 0.118602, mae: 0.474885, mean_q: 1.035710\n",
      "   86/5000: episode: 9, duration: 0.078s, episode steps:  10, steps per second: 128, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.114478, mae: 0.428918, mean_q: 1.098117\n",
      "   96/5000: episode: 10, duration: 0.108s, episode steps:  10, steps per second:  93, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.090079, mae: 0.358686, mean_q: 1.181725\n",
      "  105/5000: episode: 11, duration: 0.075s, episode steps:   9, steps per second: 120, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.079399, mae: 0.308546, mean_q: 1.316877\n",
      "  115/5000: episode: 12, duration: 0.108s, episode steps:  10, steps per second:  93, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.063544, mae: 0.262667, mean_q: 1.372165\n",
      "  126/5000: episode: 13, duration: 0.087s, episode steps:  11, steps per second: 126, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.909 [0.000, 1.000],  loss: 0.058557, mae: 0.205312, mean_q: 1.485347\n",
      "  136/5000: episode: 14, duration: 0.091s, episode steps:  10, steps per second: 110, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.900 [0.000, 1.000],  loss: 0.056564, mae: 0.161401, mean_q: 1.573874\n",
      "  145/5000: episode: 15, duration: 0.104s, episode steps:   9, steps per second:  87, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.070195, mae: 0.155071, mean_q: 1.609010\n",
      "  155/5000: episode: 16, duration: 0.166s, episode steps:  10, steps per second:  60, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.084709, mae: 0.198878, mean_q: 1.663184\n",
      "  165/5000: episode: 17, duration: 0.120s, episode steps:  10, steps per second:  83, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.059342, mae: 0.242362, mean_q: 1.703306\n",
      "  175/5000: episode: 18, duration: 0.191s, episode steps:  10, steps per second:  52, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.081694, mae: 0.321467, mean_q: 1.744228\n",
      "  183/5000: episode: 19, duration: 0.077s, episode steps:   8, steps per second: 104, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.064384, mae: 0.351625, mean_q: 1.771135\n",
      "  192/5000: episode: 20, duration: 0.080s, episode steps:   9, steps per second: 113, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.889 [0.000, 1.000],  loss: 0.059499, mae: 0.381141, mean_q: 1.839595\n",
      "  204/5000: episode: 21, duration: 0.106s, episode steps:  12, steps per second: 113, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.833 [0.000, 1.000],  loss: 0.067588, mae: 0.431507, mean_q: 1.932614\n",
      "  214/5000: episode: 22, duration: 0.088s, episode steps:  10, steps per second: 114, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.083881, mae: 0.533141, mean_q: 1.899104\n",
      "  225/5000: episode: 23, duration: 0.098s, episode steps:  11, steps per second: 112, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.909 [0.000, 1.000],  loss: 0.061176, mae: 0.552841, mean_q: 2.019708\n",
      "  236/5000: episode: 24, duration: 0.093s, episode steps:  11, steps per second: 119, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.909 [0.000, 1.000],  loss: 0.075651, mae: 0.636874, mean_q: 2.038793\n",
      "  248/5000: episode: 25, duration: 0.127s, episode steps:  12, steps per second:  94, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.833 [0.000, 1.000],  loss: 0.071840, mae: 0.694152, mean_q: 2.105995\n",
      "  258/5000: episode: 26, duration: 0.079s, episode steps:  10, steps per second: 127, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.079469, mae: 0.764757, mean_q: 2.180443\n",
      "  266/5000: episode: 27, duration: 0.087s, episode steps:   8, steps per second:  92, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.092023, mae: 0.855901, mean_q: 2.204329\n",
      "  275/5000: episode: 28, duration: 0.072s, episode steps:   9, steps per second: 125, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.889 [0.000, 1.000],  loss: 0.079455, mae: 0.913284, mean_q: 2.288585\n",
      "  284/5000: episode: 29, duration: 0.074s, episode steps:   9, steps per second: 121, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.778 [0.000, 1.000],  loss: 0.082733, mae: 0.992266, mean_q: 2.363349\n",
      "  295/5000: episode: 30, duration: 0.089s, episode steps:  11, steps per second: 123, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.727 [0.000, 1.000],  loss: 0.062738, mae: 1.009152, mean_q: 2.456933\n",
      "  304/5000: episode: 31, duration: 0.190s, episode steps:   9, steps per second:  47, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.778 [0.000, 1.000],  loss: 0.067418, mae: 1.048784, mean_q: 2.515230\n",
      "  314/5000: episode: 32, duration: 0.094s, episode steps:  10, steps per second: 107, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.900 [0.000, 1.000],  loss: 0.076993, mae: 1.072997, mean_q: 2.536979\n",
      "  325/5000: episode: 33, duration: 0.163s, episode steps:  11, steps per second:  68, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.727 [0.000, 1.000],  loss: 0.073109, mae: 1.100594, mean_q: 2.606640\n",
      "  333/5000: episode: 34, duration: 0.131s, episode steps:   8, steps per second:  61, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.875 [0.000, 1.000],  loss: 0.074153, mae: 1.149626, mean_q: 2.689179\n",
      "  343/5000: episode: 35, duration: 0.097s, episode steps:  10, steps per second: 103, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.800 [0.000, 1.000],  loss: 0.075523, mae: 1.174518, mean_q: 2.721775\n",
      "  353/5000: episode: 36, duration: 0.085s, episode steps:  10, steps per second: 118, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.800 [0.000, 1.000],  loss: 0.084128, mae: 1.177016, mean_q: 2.715151\n",
      "  363/5000: episode: 37, duration: 0.087s, episode steps:  10, steps per second: 115, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.800 [0.000, 1.000],  loss: 0.077558, mae: 1.212165, mean_q: 2.829861\n",
      "  374/5000: episode: 38, duration: 0.096s, episode steps:  11, steps per second: 115, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.818 [0.000, 1.000],  loss: 0.068938, mae: 1.243785, mean_q: 2.979501\n",
      "  383/5000: episode: 39, duration: 0.076s, episode steps:   9, steps per second: 119, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.778 [0.000, 1.000],  loss: 0.077419, mae: 1.301999, mean_q: 2.990751\n",
      "  392/5000: episode: 40, duration: 0.102s, episode steps:   9, steps per second:  88, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.889 [0.000, 1.000],  loss: 0.089396, mae: 1.338826, mean_q: 3.034433\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  401/5000: episode: 41, duration: 0.077s, episode steps:   9, steps per second: 117, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.778 [0.000, 1.000],  loss: 0.097594, mae: 1.368295, mean_q: 3.081435\n",
      "  410/5000: episode: 42, duration: 0.076s, episode steps:   9, steps per second: 119, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.889 [0.000, 1.000],  loss: 0.083567, mae: 1.388147, mean_q: 3.180958\n",
      "  420/5000: episode: 43, duration: 0.112s, episode steps:  10, steps per second:  90, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.800 [0.000, 1.000],  loss: 0.095966, mae: 1.440150, mean_q: 3.169268\n",
      "  430/5000: episode: 44, duration: 0.089s, episode steps:  10, steps per second: 112, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.700 [0.000, 1.000],  loss: 0.085012, mae: 1.519837, mean_q: 3.355239\n",
      "  439/5000: episode: 45, duration: 0.078s, episode steps:   9, steps per second: 115, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.778 [0.000, 1.000],  loss: 0.082842, mae: 1.554537, mean_q: 3.364909\n",
      "  449/5000: episode: 46, duration: 0.203s, episode steps:  10, steps per second:  49, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.800 [0.000, 1.000],  loss: 0.111372, mae: 1.583568, mean_q: 3.382428\n",
      "  460/5000: episode: 47, duration: 0.102s, episode steps:  11, steps per second: 108, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.818 [0.000, 1.000],  loss: 0.098760, mae: 1.607170, mean_q: 3.512024\n",
      "  469/5000: episode: 48, duration: 0.082s, episode steps:   9, steps per second: 110, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.778 [0.000, 1.000],  loss: 0.116663, mae: 1.650258, mean_q: 3.418687\n",
      "  477/5000: episode: 49, duration: 0.192s, episode steps:   8, steps per second:  42, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.875 [0.000, 1.000],  loss: 0.070330, mae: 1.723733, mean_q: 3.709328\n",
      "  487/5000: episode: 50, duration: 0.119s, episode steps:  10, steps per second:  84, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.700 [0.000, 1.000],  loss: 0.117558, mae: 1.743617, mean_q: 3.551327\n",
      "  497/5000: episode: 51, duration: 0.087s, episode steps:  10, steps per second: 115, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.900 [0.000, 1.000],  loss: 0.101427, mae: 1.751894, mean_q: 3.741908\n",
      "  509/5000: episode: 52, duration: 0.108s, episode steps:  12, steps per second: 111, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.750 [0.000, 1.000],  loss: 0.124433, mae: 1.709248, mean_q: 3.599266\n",
      "  517/5000: episode: 53, duration: 0.072s, episode steps:   8, steps per second: 111, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.105386, mae: 1.709949, mean_q: 3.766715\n",
      "  525/5000: episode: 54, duration: 0.066s, episode steps:   8, steps per second: 121, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 1.000 [1.000, 1.000],  loss: 0.118660, mae: 1.710337, mean_q: 3.744473\n",
      "  533/5000: episode: 55, duration: 0.100s, episode steps:   8, steps per second:  80, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.750 [0.000, 1.000],  loss: 0.103929, mae: 1.781406, mean_q: 3.979904\n",
      "  542/5000: episode: 56, duration: 0.082s, episode steps:   9, steps per second: 110, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.889 [0.000, 1.000],  loss: 0.128352, mae: 1.800627, mean_q: 3.828846\n",
      "  554/5000: episode: 57, duration: 0.103s, episode steps:  12, steps per second: 116, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.750 [0.000, 1.000],  loss: 0.115532, mae: 1.853959, mean_q: 3.929839\n",
      "  563/5000: episode: 58, duration: 0.091s, episode steps:   9, steps per second:  99, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.889 [0.000, 1.000],  loss: 0.109674, mae: 1.947471, mean_q: 4.065163\n",
      "  572/5000: episode: 59, duration: 0.108s, episode steps:   9, steps per second:  83, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.778 [0.000, 1.000],  loss: 0.107007, mae: 1.975012, mean_q: 4.051485\n",
      "  584/5000: episode: 60, duration: 0.106s, episode steps:  12, steps per second: 114, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.750 [0.000, 1.000],  loss: 0.122841, mae: 1.959293, mean_q: 3.964661\n",
      "  594/5000: episode: 61, duration: 0.212s, episode steps:  10, steps per second:  47, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.800 [0.000, 1.000],  loss: 0.109938, mae: 2.024055, mean_q: 4.169926\n",
      "  604/5000: episode: 62, duration: 0.113s, episode steps:  10, steps per second:  89, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.700 [0.000, 1.000],  loss: 0.104478, mae: 2.074483, mean_q: 4.262091\n",
      "  615/5000: episode: 63, duration: 0.086s, episode steps:  11, steps per second: 128, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.818 [0.000, 1.000],  loss: 0.116074, mae: 2.057643, mean_q: 4.208201\n",
      "  624/5000: episode: 64, duration: 0.109s, episode steps:   9, steps per second:  82, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.889 [0.000, 1.000],  loss: 0.105318, mae: 2.044777, mean_q: 4.188394\n",
      "  634/5000: episode: 65, duration: 0.160s, episode steps:  10, steps per second:  62, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.800 [0.000, 1.000],  loss: 0.115218, mae: 2.116591, mean_q: 4.276868\n",
      "  644/5000: episode: 66, duration: 0.098s, episode steps:  10, steps per second: 102, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.800 [0.000, 1.000],  loss: 0.094590, mae: 2.167484, mean_q: 4.350851\n",
      "  654/5000: episode: 67, duration: 0.091s, episode steps:  10, steps per second: 109, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.700 [0.000, 1.000],  loss: 0.101272, mae: 2.265411, mean_q: 4.440132\n",
      "  663/5000: episode: 68, duration: 0.080s, episode steps:   9, steps per second: 113, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.778 [0.000, 1.000],  loss: 0.122376, mae: 2.270668, mean_q: 4.466767\n",
      "  674/5000: episode: 69, duration: 0.103s, episode steps:  11, steps per second: 107, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.818 [0.000, 1.000],  loss: 0.091776, mae: 2.265186, mean_q: 4.504123\n",
      "  685/5000: episode: 70, duration: 0.126s, episode steps:  11, steps per second:  88, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.909 [0.000, 1.000],  loss: 0.139801, mae: 2.186002, mean_q: 4.243042\n",
      "  695/5000: episode: 71, duration: 0.076s, episode steps:  10, steps per second: 131, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.800 [0.000, 1.000],  loss: 0.094716, mae: 2.322442, mean_q: 4.653039\n",
      "  704/5000: episode: 72, duration: 0.071s, episode steps:   9, steps per second: 127, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.778 [0.000, 1.000],  loss: 0.090960, mae: 2.268028, mean_q: 4.485806\n",
      "  716/5000: episode: 73, duration: 0.092s, episode steps:  12, steps per second: 130, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.750 [0.000, 1.000],  loss: 0.085346, mae: 2.329536, mean_q: 4.686406\n",
      "  724/5000: episode: 74, duration: 0.100s, episode steps:   8, steps per second:  80, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.750 [0.000, 1.000],  loss: 0.087647, mae: 2.287666, mean_q: 4.523473\n",
      "  734/5000: episode: 75, duration: 0.087s, episode steps:  10, steps per second: 115, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.800 [0.000, 1.000],  loss: 0.084557, mae: 2.407885, mean_q: 4.708442\n",
      "  746/5000: episode: 76, duration: 0.217s, episode steps:  12, steps per second:  55, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.583 [0.000, 1.000],  loss: 0.088186, mae: 2.472422, mean_q: 4.759073\n",
      "  758/5000: episode: 77, duration: 0.102s, episode steps:  12, steps per second: 118, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.083 [0.000, 1.000],  loss: 0.090227, mae: 2.440519, mean_q: 4.712148\n",
      "  769/5000: episode: 78, duration: 0.097s, episode steps:  11, steps per second: 114, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.091 [0.000, 1.000],  loss: 0.081025, mae: 2.531078, mean_q: 4.922617\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  778/5000: episode: 79, duration: 0.169s, episode steps:   9, steps per second:  53, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.079358, mae: 2.489536, mean_q: 4.832233\n",
      "  803/5000: episode: 80, duration: 0.244s, episode steps:  25, steps per second: 103, episode reward: 25.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.680 [0.000, 1.000],  loss: 0.296426, mae: 2.625091, mean_q: 5.120111\n",
      "  814/5000: episode: 81, duration: 0.093s, episode steps:  11, steps per second: 119, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.727 [0.000, 1.000],  loss: 0.375939, mae: 2.558299, mean_q: 4.999919\n",
      "  826/5000: episode: 82, duration: 0.097s, episode steps:  12, steps per second: 124, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 0.243477, mae: 2.661238, mean_q: 5.192770\n",
      "  838/5000: episode: 83, duration: 0.130s, episode steps:  12, steps per second:  92, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 0.487322, mae: 2.725892, mean_q: 5.292162\n",
      "  849/5000: episode: 84, duration: 0.087s, episode steps:  11, steps per second: 126, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.727 [0.000, 1.000],  loss: 0.297393, mae: 2.603238, mean_q: 5.095286\n",
      "  859/5000: episode: 85, duration: 0.084s, episode steps:  10, steps per second: 119, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.800 [0.000, 1.000],  loss: 0.267741, mae: 2.675417, mean_q: 5.369616\n",
      "  868/5000: episode: 86, duration: 0.075s, episode steps:   9, steps per second: 120, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 0.130915, mae: 2.586317, mean_q: 5.182507\n",
      "  878/5000: episode: 87, duration: 0.110s, episode steps:  10, steps per second:  91, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.700 [0.000, 1.000],  loss: 0.259006, mae: 2.648613, mean_q: 5.187301\n",
      "  889/5000: episode: 88, duration: 0.147s, episode steps:  11, steps per second:  75, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.636 [0.000, 1.000],  loss: 0.374359, mae: 2.779153, mean_q: 5.400675\n",
      "  902/5000: episode: 89, duration: 0.169s, episode steps:  13, steps per second:  77, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.692 [0.000, 1.000],  loss: 0.237899, mae: 2.729826, mean_q: 5.281416\n",
      "  912/5000: episode: 90, duration: 0.089s, episode steps:  10, steps per second: 113, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.600 [0.000, 1.000],  loss: 0.451937, mae: 2.762586, mean_q: 5.307168\n",
      "  922/5000: episode: 91, duration: 0.080s, episode steps:  10, steps per second: 126, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.700 [0.000, 1.000],  loss: 0.238227, mae: 2.761685, mean_q: 5.393025\n",
      "  932/5000: episode: 92, duration: 0.169s, episode steps:  10, steps per second:  59, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.700 [0.000, 1.000],  loss: 0.106538, mae: 2.811806, mean_q: 5.486478\n",
      "  961/5000: episode: 93, duration: 0.290s, episode steps:  29, steps per second: 100, episode reward: 29.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.310 [0.000, 1.000],  loss: 0.159793, mae: 2.816108, mean_q: 5.429102\n",
      "  972/5000: episode: 94, duration: 0.089s, episode steps:  11, steps per second: 124, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.273 [0.000, 1.000],  loss: 0.271418, mae: 2.902323, mean_q: 5.590270\n",
      "  980/5000: episode: 95, duration: 0.067s, episode steps:   8, steps per second: 120, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.086124, mae: 2.986259, mean_q: 5.831429\n",
      "  990/5000: episode: 96, duration: 0.124s, episode steps:  10, steps per second:  80, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.270557, mae: 3.059471, mean_q: 5.961783\n",
      " 1001/5000: episode: 97, duration: 0.102s, episode steps:  11, steps per second: 108, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.273 [0.000, 1.000],  loss: 0.601433, mae: 3.147521, mean_q: 6.059153\n",
      " 1025/5000: episode: 98, duration: 0.194s, episode steps:  24, steps per second: 124, episode reward: 24.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.625 [0.000, 1.000],  loss: 0.640211, mae: 3.080703, mean_q: 5.812641\n",
      " 1034/5000: episode: 99, duration: 0.095s, episode steps:   9, steps per second:  95, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.667 [0.000, 1.000],  loss: 1.186850, mae: 3.124055, mean_q: 5.896552\n",
      " 1062/5000: episode: 100, duration: 0.325s, episode steps:  28, steps per second:  86, episode reward: 28.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.429 [0.000, 1.000],  loss: 0.549807, mae: 3.121039, mean_q: 5.874294\n",
      " 1077/5000: episode: 101, duration: 0.118s, episode steps:  15, steps per second: 127, episode reward: 15.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.467 [0.000, 1.000],  loss: 0.543511, mae: 3.260549, mean_q: 6.176616\n",
      " 1090/5000: episode: 102, duration: 0.163s, episode steps:  13, steps per second:  80, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.385 [0.000, 1.000],  loss: 0.230684, mae: 3.299114, mean_q: 6.340535\n",
      " 1099/5000: episode: 103, duration: 0.135s, episode steps:   9, steps per second:  66, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.111 [0.000, 1.000],  loss: 0.688172, mae: 3.322583, mean_q: 6.325643\n",
      " 1108/5000: episode: 104, duration: 0.084s, episode steps:   9, steps per second: 107, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.839067, mae: 3.514184, mean_q: 6.666213\n",
      " 1118/5000: episode: 105, duration: 0.088s, episode steps:  10, steps per second: 113, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.200 [0.000, 1.000],  loss: 0.700408, mae: 3.438080, mean_q: 6.493634\n",
      " 1128/5000: episode: 106, duration: 0.081s, episode steps:  10, steps per second: 123, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.300 [0.000, 1.000],  loss: 0.576056, mae: 3.337714, mean_q: 6.266527\n",
      " 1138/5000: episode: 107, duration: 0.114s, episode steps:  10, steps per second:  88, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.100 [0.000, 1.000],  loss: 0.653323, mae: 3.342411, mean_q: 6.267422\n",
      " 1147/5000: episode: 108, duration: 0.075s, episode steps:   9, steps per second: 120, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.775749, mae: 3.638855, mean_q: 6.938223\n",
      " 1156/5000: episode: 109, duration: 0.075s, episode steps:   9, steps per second: 121, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.771306, mae: 3.455224, mean_q: 6.456459\n",
      " 1165/5000: episode: 110, duration: 0.076s, episode steps:   9, steps per second: 119, episode reward:  9.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.111 [0.000, 1.000],  loss: 1.433759, mae: 3.669243, mean_q: 6.803744\n",
      " 1173/5000: episode: 111, duration: 0.066s, episode steps:   8, steps per second: 121, episode reward:  8.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.000 [0.000, 0.000],  loss: 0.727139, mae: 3.737942, mean_q: 7.008755\n",
      " 1187/5000: episode: 112, duration: 0.143s, episode steps:  14, steps per second:  98, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.214 [0.000, 1.000],  loss: 0.777394, mae: 3.702110, mean_q: 6.999641\n",
      " 1197/5000: episode: 113, duration: 0.177s, episode steps:  10, steps per second:  56, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.100 [0.000, 1.000],  loss: 0.907627, mae: 3.741426, mean_q: 7.074940\n",
      " 1208/5000: episode: 114, duration: 0.089s, episode steps:  11, steps per second: 124, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.364 [0.000, 1.000],  loss: 0.804346, mae: 3.720288, mean_q: 7.017368\n",
      " 1218/5000: episode: 115, duration: 0.081s, episode steps:  10, steps per second: 123, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.200 [0.000, 1.000],  loss: 1.068764, mae: 3.737558, mean_q: 6.954504\n",
      " 1229/5000: episode: 116, duration: 0.087s, episode steps:  11, steps per second: 126, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.182 [0.000, 1.000],  loss: 0.977300, mae: 3.845490, mean_q: 7.166476\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1239/5000: episode: 117, duration: 0.100s, episode steps:  10, steps per second: 100, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.200 [0.000, 1.000],  loss: 0.921042, mae: 3.818846, mean_q: 7.178374\n",
      " 1250/5000: episode: 118, duration: 0.183s, episode steps:  11, steps per second:  60, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.273 [0.000, 1.000],  loss: 0.868373, mae: 3.894866, mean_q: 7.296629\n",
      " 1260/5000: episode: 119, duration: 0.084s, episode steps:  10, steps per second: 118, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.300 [0.000, 1.000],  loss: 0.796674, mae: 3.788609, mean_q: 7.097714\n",
      " 1272/5000: episode: 120, duration: 0.099s, episode steps:  12, steps per second: 121, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 0.575925, mae: 3.901051, mean_q: 7.410100\n",
      " 1283/5000: episode: 121, duration: 0.094s, episode steps:  11, steps per second: 117, episode reward: 11.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.273 [0.000, 1.000],  loss: 0.988523, mae: 3.953587, mean_q: 7.463334\n",
      " 1293/5000: episode: 122, duration: 0.112s, episode steps:  10, steps per second:  89, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.300 [0.000, 1.000],  loss: 1.240755, mae: 4.054111, mean_q: 7.385884\n",
      " 1305/5000: episode: 123, duration: 0.097s, episode steps:  12, steps per second: 123, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.333 [0.000, 1.000],  loss: 0.652492, mae: 3.884814, mean_q: 7.357000\n",
      " 1315/5000: episode: 124, duration: 0.079s, episode steps:  10, steps per second: 127, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.200 [0.000, 1.000],  loss: 1.053999, mae: 4.160573, mean_q: 7.833081\n",
      " 1328/5000: episode: 125, duration: 0.109s, episode steps:  13, steps per second: 120, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 0.826115, mae: 4.157003, mean_q: 7.848609\n",
      " 1344/5000: episode: 126, duration: 0.276s, episode steps:  16, steps per second:  58, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.438 [0.000, 1.000],  loss: 0.622429, mae: 3.970533, mean_q: 7.523159\n",
      " 1357/5000: episode: 127, duration: 0.105s, episode steps:  13, steps per second: 124, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 0.962266, mae: 4.076957, mean_q: 7.636106\n",
      " 1374/5000: episode: 128, duration: 0.135s, episode steps:  17, steps per second: 125, episode reward: 17.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 0.579662, mae: 4.130039, mean_q: 7.818071\n",
      " 1388/5000: episode: 129, duration: 0.111s, episode steps:  14, steps per second: 126, episode reward: 14.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 0.589298, mae: 4.148253, mean_q: 7.932481\n",
      " 1404/5000: episode: 130, duration: 0.232s, episode steps:  16, steps per second:  69, episode reward: 16.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.438 [0.000, 1.000],  loss: 1.038247, mae: 4.265627, mean_q: 7.991699\n",
      " 1424/5000: episode: 131, duration: 0.154s, episode steps:  20, steps per second: 130, episode reward: 20.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.550 [0.000, 1.000],  loss: 0.869443, mae: 4.179709, mean_q: 7.828191\n",
      " 1436/5000: episode: 132, duration: 0.124s, episode steps:  12, steps per second:  97, episode reward: 12.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.417 [0.000, 1.000],  loss: 1.195418, mae: 4.217063, mean_q: 7.953274\n",
      " 1446/5000: episode: 133, duration: 0.086s, episode steps:  10, steps per second: 116, episode reward: 10.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.300 [0.000, 1.000],  loss: 0.875811, mae: 4.349639, mean_q: 8.285629\n",
      " 1459/5000: episode: 134, duration: 0.101s, episode steps:  13, steps per second: 129, episode reward: 13.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.462 [0.000, 1.000],  loss: 0.718397, mae: 4.320494, mean_q: 8.277596\n",
      " 1572/5000: episode: 135, duration: 1.144s, episode steps: 113, steps per second:  99, episode reward: 113.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.504 [0.000, 1.000],  loss: 1.073463, mae: 4.452042, mean_q: 8.357518\n",
      " 1594/5000: episode: 136, duration: 0.177s, episode steps:  22, steps per second: 124, episode reward: 22.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  loss: 0.655105, mae: 4.607748, mean_q: 8.782789\n",
      " 1627/5000: episode: 137, duration: 0.286s, episode steps:  33, steps per second: 116, episode reward: 33.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  loss: 0.815356, mae: 4.699773, mean_q: 8.961699\n",
      " 1671/5000: episode: 138, duration: 0.474s, episode steps:  44, steps per second:  93, episode reward: 44.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.545 [0.000, 1.000],  loss: 0.828321, mae: 4.678611, mean_q: 8.925770\n",
      " 1742/5000: episode: 139, duration: 0.646s, episode steps:  71, steps per second: 110, episode reward: 71.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.507 [0.000, 1.000],  loss: 0.997648, mae: 4.827485, mean_q: 9.189942\n",
      " 1772/5000: episode: 140, duration: 0.242s, episode steps:  30, steps per second: 124, episode reward: 30.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.533 [0.000, 1.000],  loss: 0.818272, mae: 5.015326, mean_q: 9.579916\n",
      " 1825/5000: episode: 141, duration: 0.499s, episode steps:  53, steps per second: 106, episode reward: 53.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.509 [0.000, 1.000],  loss: 1.162359, mae: 5.157239, mean_q: 9.807308\n",
      " 1879/5000: episode: 142, duration: 0.515s, episode steps:  54, steps per second: 105, episode reward: 54.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.500 [0.000, 1.000],  loss: 1.014925, mae: 5.127289, mean_q: 9.802826\n",
      " 1917/5000: episode: 143, duration: 0.338s, episode steps:  38, steps per second: 112, episode reward: 38.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 1.047647, mae: 5.359536, mean_q: 10.253789\n",
      " 2021/5000: episode: 144, duration: 0.932s, episode steps: 104, steps per second: 112, episode reward: 104.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  loss: 1.031196, mae: 5.512574, mean_q: 10.616120\n",
      " 2074/5000: episode: 145, duration: 0.525s, episode steps:  53, steps per second: 101, episode reward: 53.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.528 [0.000, 1.000],  loss: 1.166045, mae: 5.772858, mean_q: 11.135741\n",
      " 2125/5000: episode: 146, duration: 0.395s, episode steps:  51, steps per second: 129, episode reward: 51.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.529 [0.000, 1.000],  loss: 1.287249, mae: 5.909251, mean_q: 11.369691\n",
      " 2268/5000: episode: 147, duration: 1.340s, episode steps: 143, steps per second: 107, episode reward: 143.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.517 [0.000, 1.000],  loss: 1.005384, mae: 6.124899, mean_q: 11.940000\n",
      " 2386/5000: episode: 148, duration: 1.150s, episode steps: 118, steps per second: 103, episode reward: 118.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.517 [0.000, 1.000],  loss: 0.935886, mae: 6.589620, mean_q: 12.940606\n",
      " 2487/5000: episode: 149, duration: 0.987s, episode steps: 101, steps per second: 102, episode reward: 101.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  loss: 1.222916, mae: 6.932660, mean_q: 13.609054\n",
      " 2597/5000: episode: 150, duration: 0.982s, episode steps: 110, steps per second: 112, episode reward: 110.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.509 [0.000, 1.000],  loss: 1.553730, mae: 7.212391, mean_q: 14.121026\n",
      " 2697/5000: episode: 151, duration: 0.956s, episode steps: 100, steps per second: 105, episode reward: 100.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.530 [0.000, 1.000],  loss: 1.079139, mae: 7.561040, mean_q: 14.944355\n",
      " 2828/5000: episode: 152, duration: 1.261s, episode steps: 131, steps per second: 104, episode reward: 131.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.519 [0.000, 1.000],  loss: 1.498537, mae: 7.781649, mean_q: 15.348847\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2946/5000: episode: 153, duration: 1.049s, episode steps: 118, steps per second: 112, episode reward: 118.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.525 [0.000, 1.000],  loss: 1.710214, mae: 8.210090, mean_q: 16.227016\n",
      " 3083/5000: episode: 154, duration: 1.387s, episode steps: 137, steps per second:  99, episode reward: 137.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 1.210118, mae: 8.545197, mean_q: 17.052246\n",
      " 3226/5000: episode: 155, duration: 1.416s, episode steps: 143, steps per second: 101, episode reward: 143.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.524 [0.000, 1.000],  loss: 1.464604, mae: 8.988456, mean_q: 17.970795\n",
      " 3380/5000: episode: 156, duration: 1.353s, episode steps: 154, steps per second: 114, episode reward: 154.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 1.291879, mae: 9.555615, mean_q: 19.244743\n",
      " 3517/5000: episode: 157, duration: 1.498s, episode steps: 137, steps per second:  91, episode reward: 137.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.526 [0.000, 1.000],  loss: 1.425919, mae: 10.074527, mean_q: 20.334032\n",
      " 3699/5000: episode: 158, duration: 1.719s, episode steps: 182, steps per second: 106, episode reward: 182.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.511 [0.000, 1.000],  loss: 1.517264, mae: 10.616029, mean_q: 21.444994\n",
      " 3899/5000: episode: 159, duration: 1.838s, episode steps: 200, steps per second: 109, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  loss: 1.567232, mae: 11.307537, mean_q: 22.969326\n",
      " 4099/5000: episode: 160, duration: 1.870s, episode steps: 200, steps per second: 107, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.515 [0.000, 1.000],  loss: 1.549391, mae: 12.173736, mean_q: 24.744780\n",
      " 4299/5000: episode: 161, duration: 1.807s, episode steps: 200, steps per second: 111, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  loss: 1.781793, mae: 12.943020, mean_q: 26.407476\n",
      " 4499/5000: episode: 162, duration: 1.918s, episode steps: 200, steps per second: 104, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  loss: 1.912834, mae: 13.709712, mean_q: 27.999043\n",
      " 4699/5000: episode: 163, duration: 1.771s, episode steps: 200, steps per second: 113, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  loss: 2.782630, mae: 14.467058, mean_q: 29.417597\n",
      " 4899/5000: episode: 164, duration: 1.932s, episode steps: 200, steps per second: 103, episode reward: 200.000, mean reward:  1.000 [ 1.000,  1.000], mean action: 0.510 [0.000, 1.000],  loss: 2.537938, mae: 15.200526, mean_q: 30.904526\n",
      "done, took 49.726 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2bb636d9fd0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy = EpsGreedyQPolicy()\n",
    "memory = SequentialMemory(limit=50000, window_length=1)\n",
    "dqn = DQNAgent(model=model, nb_actions=nb_actions, memory=memory, nb_steps_warmup=10,\n",
    "target_model_update=1e-2, policy=policy)\n",
    "dqn.compile(Adam(lr=1e-3), metrics=['mae'])\n",
    "\n",
    "# Okay, now it's time to learn something! We visualize the training here for show, but this slows down training quite a lot. \n",
    "dqn.fit(env, nb_steps=5000, visualize=False, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 5 episodes ...\n",
      "Episode 1: reward: 200.000, steps: 200\n",
      "Episode 2: reward: 200.000, steps: 200\n",
      "Episode 3: reward: 200.000, steps: 200\n",
      "Episode 4: reward: 200.000, steps: 200\n",
      "Episode 5: reward: 200.000, steps: 200\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2bb636d9400>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqn.test(env, nb_episodes=5, visualize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARNING] savedModels/weights.h5f.index already exists - overwrite? [y/n]y\n",
      "[TIP] Next time specify overwrite=True!\n"
     ]
    }
   ],
   "source": [
    "dqn.save_weights('savedModels/weights.h5f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.load_weights('savedModels/weights.h5f')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f3807dd5efc34377faf417a9cd79d1987ca34792fc2d643ce72032a592f034d6"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
